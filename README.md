# machine_learning
TP machine learning - Reconnaissance d'animaux avec Python et r√©seaux de neurones

## üìã √âtat d'avancement du projet

### ‚úÖ √âtapes compl√©t√©es

- [x] **√âtape 1 : Collecte et pr√©paration des donn√©es**
  - Structure de dossiers cr√©√©e pour l'organisation des images par classe
  - Organisation : `/data/tigres/`, `/data/elephants/`, `/data/giraffes/`
  - Le syst√®me de labellage par dossier est en place (suffisant pour TensorFlow/Keras)
  - Dossier `/data/extra/` cr√©√© pour stocker les images suppl√©mentaires (ignor√© automatiquement)

- [x] **√âtape 2 : Pr√©traitement essentiel**
  - ‚úÖ Redimensionnement uniforme des images (224x224 par d√©faut)
  - ‚úÖ Normalisation des valeurs de pixels (0-1)
  - ‚úÖ Augmentation des donn√©es :
    - Rotation (jusqu'√† 359¬∞)
    - Zoom (jusqu'√† 20%)
    - Retournement horizontal
    - Translation (d√©placement horizontal/vertical)
  - ‚úÖ Pr√©traitement √† la vol√©e (en m√©moire) via les g√©n√©rateurs
  - Script `preprocessing.py` cr√©√© et fonctionnel

### ‚úÖ √âtapes compl√©t√©es (suite)

- [x] **√âtape 3 : Construction du mod√®le CNN**
  - ‚úÖ Architecture du r√©seau de neurones compl√®te
  - ‚úÖ 4 blocs de couches convolutionnelles (32, 64, 128, 256 filtres)
  - ‚úÖ Couches de pooling (MaxPooling2D) pour r√©duction dimensionnelle
  - ‚úÖ Couches denses (512, 256 neurones) avec Dropout
  - ‚úÖ Sortie softmax pour probabilit√©s par classe
  - ‚úÖ BatchNormalization pour stabiliser l'entra√Ænement
  - ‚úÖ Callbacks (EarlyStopping, ReduceLROnPlateau)
  - Script `model.py` cr√©√© et fonctionnel

### ‚úÖ √âtapes compl√©t√©es (suite)

- [x] **√âtape 4 : Entra√Ænement et validation**
  - ‚úÖ S√©paration train/validation (80/20)
  - ‚úÖ Optimiseur Adam avec learning rate adaptatif
  - ‚úÖ Fonction de perte : categorical_crossentropy
  - ‚úÖ Pr√©vention du surapprentissage :
    - Dropout dans les couches denses
    - Augmentation des donn√©es
    - BatchNormalization
    - EarlyStopping
    - ReduceLROnPlateau
  - ‚úÖ Sauvegarde du meilleur mod√®le
  - ‚úÖ Visualisation des courbes d'entra√Ænement
  - Script `train.py` cr√©√© et fonctionnel

### üîÑ En cours

### ‚è≥ √Ä faire

- [ ] **√âtape 5 : √âvaluation et am√©lioration**
  - M√©triques de performance (pr√©cision, rappel, matrice de confusion)
  - Analyse des r√©sultats
  - Optimisation du mod√®le

---

## üìÅ Structure du projet

```
machine_learning/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ tigre/         # Images de tigres
‚îÇ   ‚îú‚îÄ‚îÄ elephant/      # Images d'√©l√©phants
‚îÇ   ‚îú‚îÄ‚îÄ giraffe/       # Images de girafes
‚îÇ   ‚îî‚îÄ‚îÄ extra/         # Images suppl√©mentaires (ignor√© lors du traitement)
‚îú‚îÄ‚îÄ models/            # Mod√®les sauvegard√©s (cr√©√© apr√®s entra√Ænement)
‚îÇ   ‚îî‚îÄ‚îÄ best_model.h5  # Meilleur mod√®le sauvegard√©
‚îú‚îÄ‚îÄ preprocessing.py   # Script de pr√©traitement des images ‚úÖ
‚îú‚îÄ‚îÄ model.py           # Script de construction du mod√®le CNN ‚úÖ
‚îú‚îÄ‚îÄ train.py           # Script d'entra√Ænement et validation ‚úÖ
‚îú‚îÄ‚îÄ visualize.py       # Script de visualisation de l'augmentation ‚úÖ
‚îú‚îÄ‚îÄ requirements.txt   # D√©pendances Python ‚úÖ
‚îú‚îÄ‚îÄ README.md          # Ce fichier
‚îî‚îÄ‚îÄ TP-Reconnaissance-danimaux-avec-Python-et-reseaux-de-neurones.pdf
```

---

## üöÄ Installation

1. Installer les d√©pendances:
```bash
pip install -r requirements.txt
```

Les d√©pendances incluent :
- `tensorflow>=2.10.0` - Framework pour les r√©seaux de neurones
- `numpy>=1.21.0` - Calculs num√©riques
- `Pillow>=9.0.0` - Traitement d'images
- `matplotlib>=3.5.0` - Visualisation

---

## üìä Pr√©traitement des images

### Fonctionnalit√©s impl√©ment√©es

Le script `preprocessing.py` contient :

1. **Classe `ImagePreprocessor`**
   - Redimensionnement uniforme
   - Normalisation des pixels

2. **Fonction `create_data_generators()`**
   - Cr√©ation automatique des g√©n√©rateurs d'entra√Ænement et de validation
   - Application de l'augmentation des donn√©es
   - S√©paration automatique train/validation (80/20)
   - **Ignoration automatique du dossier `extra/`**

3. **Fonction `visualize_augmentation()`**
   - Visualisation des effets de l'augmentation
   - **Ignoration automatique du dossier `extra/`**
   - G√©n√®re une image `augmentation_examples.png` avec des exemples

4. **Fonction `preprocess_dataset()`**
   - Pr√©traitement en lot de toutes les images (optionnel, pour sauvegarde manuelle si n√©cessaire)
   - **Ignoration automatique du dossier `extra/`**
   - Note: Les images sont pr√©trait√©es √† la vol√©e par les g√©n√©rateurs (pas de sauvegarde par d√©faut)

### Utilisation

```python
from preprocessing import create_data_generators

# Cr√©er les g√©n√©rateurs de donn√©es avec augmentation
train_gen, val_gen, classes = create_data_generators(
    data_dir="data",
    target_size=(224, 224),
    batch_size=32,
    validation_split=0.2
)

# Les g√©n√©rateurs sont pr√™ts pour l'entra√Ænement du mod√®le CNN
print(f"Classes d√©tect√©es: {classes}")
print(f"Images d'entra√Ænement: {train_gen.samples}")
print(f"Images de validation: {val_gen.samples}")
```

### Visualisation de l'augmentation

Pour visualiser les effets de l'augmentation des donn√©es, utilisez le script `visualize.py` :

```bash
python visualize.py
```

Ou directement en Python :

```python
from preprocessing import visualize_augmentation

visualize_augmentation(
    data_dir="data",
    target_size=(224, 224),
    num_samples=4  # Nombre d'exemples √† afficher
)
```

Cela affichera une fen√™tre avec des exemples d'images originales et augment√©es, et sauvegardera `augmentation_examples.png`.

### Param√®tres d'augmentation configur√©s

- **Rotation** : ¬±359 degr√©s
- **Zoom** : ¬±20%
- **Retournement horizontal** : Activ√©
- **Translation** : ¬±10% horizontal et vertical
- **Normalisation** : Valeurs entre 0 et 1

---

## üß† Construction du mod√®le CNN

### Architecture impl√©ment√©e

Le script `model.py` contient un mod√®le CNN complet avec :

1. **4 Blocs de couches convolutionnelles**
   - Bloc 1 : 32 filtres (3x3) + BatchNormalization + MaxPooling
   - Bloc 2 : 64 filtres (3x3) + BatchNormalization + MaxPooling
   - Bloc 3 : 128 filtres (3x3) + BatchNormalization + MaxPooling
   - Bloc 4 : 256 filtres (3x3) + BatchNormalization + MaxPooling

2. **Couches denses**
   - Dense 1 : 512 neurones + Dropout (0.5)
   - Dense 2 : 256 neurones + Dropout (0.3)
   - Sortie : 3 neurones avec activation softmax

3. **Fonctionnalit√©s**
   - BatchNormalization pour stabiliser l'entra√Ænement
   - Dropout pour pr√©venir le surapprentissage
   - Optimiseur Adam avec learning rate adaptatif
   - Callbacks : EarlyStopping et ReduceLROnPlateau

### Utilisation

```python
from model import build_cnn_model, compile_model, create_callbacks
from preprocessing import create_data_generators

# Cr√©er les g√©n√©rateurs de donn√©es
train_gen, val_gen, classes = create_data_generators("data")

# Construire le mod√®le
model = build_cnn_model(
    input_shape=(224, 224, 3),
    num_classes=len(classes)
)

# Compiler le mod√®le
model = compile_model(model, learning_rate=0.001)

# Afficher le r√©sum√©
model.summary()
```

### Ex√©cution du script

```bash
python model.py
```

Cela affichera :
- L'architecture compl√®te du mod√®le
- Le nombre de param√®tres
- Les callbacks configur√©s
- Un graphique de l'architecture (si graphviz install√©)

---

## üöÇ Entra√Ænement et validation du mod√®le

### Fonctionnalit√©s impl√©ment√©es

Le script `train.py` g√®re l'entra√Ænement complet avec :

1. **S√©paration des donn√©es**
   - 80% pour l'entra√Ænement
   - 20% pour la validation
   - S√©paration automatique via `validation_split`

2. **Optimisation**
   - **Optimiseur** : Adam (Adaptive Moment Estimation)
   - **Learning rate** : 0.001 (adaptatif avec ReduceLROnPlateau)
   - **Fonction de perte** : categorical_crossentropy (entropie crois√©e)
   - **M√©triques** : accuracy, top_k_categorical_accuracy

3. **Pr√©vention du surapprentissage**
   - ‚úÖ **Augmentation des donn√©es** : rotation, zoom, retournement
   - ‚úÖ **Dropout** : 0.5 et 0.3 dans les couches denses
   - ‚úÖ **BatchNormalization** : normalisation apr√®s chaque couche conv
   - ‚úÖ **EarlyStopping** : arr√™t si pas d'am√©lioration pendant 10 √©poques
   - ‚úÖ **ReduceLROnPlateau** : r√©duction du learning rate si plateau

4. **Sauvegarde et suivi**
   - Sauvegarde automatique du meilleur mod√®le (`models/best_model.h5`)
   - Historique CSV (`training_history.csv`)
   - Graphiques de performance (`training_curves.png`)

### Utilisation

```bash
python train.py
```

Ou en Python :

```python
from train import train_model, plot_training_history

# Entra√Æner le mod√®le
model, history = train_model(
    data_dir="data",
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    learning_rate=0.001
)

# Visualiser les r√©sultats
plot_training_history(history)
```

### Param√®tres d'entra√Ænement

- **√âpoques** : 50 (avec arr√™t anticip√© si n√©cessaire)
- **Batch size** : 32
- **Validation split** : 0.2 (20%)
- **Learning rate** : 0.001 (adaptatif)

### Fichiers g√©n√©r√©s

Apr√®s l'entra√Ænement, vous obtiendrez :
- `models/best_model.h5` : Meilleur mod√®le sauvegard√©
- `training_history.csv` : Historique d√©taill√© de chaque √©poque
- `training_curves.png` : Graphiques de pr√©cision et perte

---

## üìù Notes

- **Objectif de pr√©cision** : Le mod√®le doit atteindre au moins 66% de pr√©cision pour √™tre valid√©
- **Classes** : 3 classes (tigres, √©l√©phants, girafes)
- **Format des images** : JPG, PNG, JPEG accept√©s
- **Taille standard** : 224x224 pixels (compatible avec les mod√®les pr√©-entra√Æn√©s)

---

## üîÑ Derni√®re mise √† jour

- ‚úÖ Pr√©traitement des images impl√©ment√© (redimensionnement, normalisation, augmentation)
- ‚úÖ Structure du projet organis√©e
- ‚úÖ Documentation initiale cr√©√©e
- ‚úÖ Dossier `extra/` ignor√© automatiquement lors du traitement
- ‚úÖ Pr√©traitement √† la vol√©e (pas de sauvegarde, traitement en m√©moire pendant l'entra√Ænement)
- ‚úÖ Script `visualize.py` cr√©√© pour visualiser l'augmentation des donn√©es
- ‚úÖ Correction de la fonction `visualize_augmentation()` (compatibilit√© Python 3)
- ‚úÖ Mod√®le CNN construit avec 4 blocs convolutionnels, couches denses, et callbacks
- ‚úÖ **Script d'entra√Ænement complet** avec s√©paration des donn√©es, optimisation, et pr√©vention du surapprentissage